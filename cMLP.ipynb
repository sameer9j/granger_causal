{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_VWP_t_14</th>\n",
       "      <th>MSFT_VWP_t_14</th>\n",
       "      <th>FB_VWP_t_14</th>\n",
       "      <th>TWTR_VWP_t_14</th>\n",
       "      <th>AAPL_VWP_t_13</th>\n",
       "      <th>MSFT_VWP_t_13</th>\n",
       "      <th>FB_VWP_t_13</th>\n",
       "      <th>TWTR_VWP_t_13</th>\n",
       "      <th>AAPL_VWP_t_12</th>\n",
       "      <th>MSFT_VWP_t_12</th>\n",
       "      <th>...</th>\n",
       "      <th>FB_VWP_t_3</th>\n",
       "      <th>TWTR_VWP_t_3</th>\n",
       "      <th>AAPL_VWP_t_2</th>\n",
       "      <th>MSFT_VWP_t_2</th>\n",
       "      <th>FB_VWP_t_2</th>\n",
       "      <th>TWTR_VWP_t_2</th>\n",
       "      <th>AAPL_VWP_t_1</th>\n",
       "      <th>MSFT_VWP_t_1</th>\n",
       "      <th>FB_VWP_t_1</th>\n",
       "      <th>TWTR_VWP_t_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.009829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.009832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AAPL_VWP_t_14  MSFT_VWP_t_14  FB_VWP_t_14  TWTR_VWP_t_14  AAPL_VWP_t_13  \\\n",
       "0       0.008251       0.009330     0.007926       0.009827       0.008258   \n",
       "1       0.008258       0.009330     0.007946       0.009877       0.008258   \n",
       "2       0.008258       0.009334     0.007939       0.009871       0.008267   \n",
       "3       0.008267       0.009351     0.007956       0.009848       0.008273   \n",
       "4       0.008273       0.009342     0.007944       0.009855       0.008267   \n",
       "\n",
       "   MSFT_VWP_t_13  FB_VWP_t_13  TWTR_VWP_t_13  AAPL_VWP_t_12  MSFT_VWP_t_12  \\\n",
       "0       0.009330     0.007946       0.009876       0.008258       0.009333   \n",
       "1       0.009333     0.007939       0.009870       0.008267       0.009350   \n",
       "2       0.009351     0.007955       0.009848       0.008273       0.009342   \n",
       "3       0.009342     0.007944       0.009854       0.008267       0.009313   \n",
       "4       0.009313     0.007932       0.009803       0.008227       0.009312   \n",
       "\n",
       "       ...       FB_VWP_t_3  TWTR_VWP_t_3  AAPL_VWP_t_2  MSFT_VWP_t_2  \\\n",
       "0      ...         0.007962      0.009804      0.008263      0.009306   \n",
       "1      ...         0.007981      0.009851      0.008258      0.009303   \n",
       "2      ...         0.007980      0.009848      0.008255      0.009280   \n",
       "3      ...         0.007978      0.009840      0.008254      0.009281   \n",
       "4      ...         0.007973      0.009839      0.008257      0.009284   \n",
       "\n",
       "   FB_VWP_t_2  TWTR_VWP_t_2  AAPL_VWP_t_1  MSFT_VWP_t_1  FB_VWP_t_1  \\\n",
       "0    0.007981      0.009851      0.008257      0.009302    0.007980   \n",
       "1    0.007980      0.009847      0.008254      0.009280    0.007978   \n",
       "2    0.007978      0.009840      0.008253      0.009281    0.007973   \n",
       "3    0.007973      0.009839      0.008257      0.009283    0.007969   \n",
       "4    0.007970      0.009830      0.008260      0.009288    0.007976   \n",
       "\n",
       "   TWTR_VWP_t_1  \n",
       "0      0.009847  \n",
       "1      0.009839  \n",
       "2      0.009838  \n",
       "3      0.009829  \n",
       "4      0.009832  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8642, 56)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('tech_60m.txt')\n",
    "# get features and convert to array so that we can feed into model\n",
    "data_x = data.iloc[:, 1:57]\n",
    "data_x.head()\n",
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.009279\n",
       "1    0.009280\n",
       "2    0.009283\n",
       "3    0.009287\n",
       "4    0.009292\n",
       "Name: MSFT_VWP, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8642,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get actual y values and convert to array \n",
    "data_y = data.iloc[:, 58]\n",
    "data_y.head()\n",
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_s1_values = data_x.filter(like='MSFT').values\n",
    "X_s2_values = data_x.filter(like='AAPL').values\n",
    "X_s3_values = data_x.filter(like='FB').values\n",
    "X_s4_values = data_x.filter(like='TWTR').values\n",
    "Y_s1_values = data_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = X_s1_values.shape[0] # number of rows\n",
    "n_lags = 14 # number of lags\n",
    "lambda_value = 0.1 # hyperparam\n",
    "\n",
    "n_hidden1 = 8\n",
    "n_hidden2 = 100\n",
    "\n",
    "n_half_hidden1 = int(n_hidden1 / 2)\n",
    "\n",
    "n_outputs = 1\n",
    "\n",
    "# lambda_val = tf.fill(tf.get_variable(name='lambda', shape=(n_lags)), lambda_value)\n",
    "lambda_val = tf.fill((1, 1), lambda_value)\n",
    "X_s1 = tf.placeholder(tf.float32, shape=(None, n_lags), name=\"X_s1\")\n",
    "X_s2 = tf.placeholder(tf.float32, shape=(None, n_lags), name=\"X_s2\")#changed\n",
    "X_s3 = tf.placeholder(tf.float32, shape=(None, n_lags), name=\"X_s3\")#changed\n",
    "X_s4 = tf.placeholder(tf.float32, shape=(None, n_lags), name=\"X_s4\")#changed\n",
    "\n",
    "y_s1 = tf.placeholder(tf.float32, shape=(None), name=\"y_s1\")\n",
    "# y_s2 = tf.placeholder(tf.int32, shape=(n_inputs), name=\"y_s2\")\n",
    "\n",
    "w_s1 = tf.get_variable(name='w_s1', shape=(n_lags, n_half_hidden1), initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "w_s2 = tf.get_variable(name='w_s2', shape=(n_lags, n_half_hidden1), initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "w_s3 = tf.get_variable(name='w_s3', shape=(n_lags, n_half_hidden1), initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "w_s4 = tf.get_variable(name='w_s4', shape=(n_lags, n_half_hidden1), initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "\n",
    "b_s1 = tf.get_variable(name='b_s1', shape=(n_half_hidden1), initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "b_s2 = tf.get_variable(name='b_s2', shape=(n_half_hidden1), initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "b_s3 = tf.get_variable(name='b_s3', shape=(n_half_hidden1), initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "b_s4 = tf.get_variable(name='b_s4', shape=(n_half_hidden1), initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    s1_hidden1 = tf.nn.relu(tf.matmul(X_s1, w_s1) + b_s1)\n",
    "    s2_hidden1 = tf.nn.relu(tf.matmul(X_s2, w_s2) + b_s2)\n",
    "    s3_hidden1 = tf.nn.relu(tf.matmul(X_s3, w_s3) + b_s3)\n",
    "    s4_hidden1 = tf.nn.relu(tf.matmul(X_s4, w_s4) + b_s4)\n",
    "\n",
    "    hidden2 = tf.layers.dense(tf.concat([s1_hidden1, s2_hidden1, s3_hidden1, s4_hidden1], 1), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "#     hidden2 = tf.layers.dense(s2_hidden1, n_hidden2, name=\"hidden2\",\n",
    "#                               activation=tf.nn.relu)\n",
    "\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "\n",
    "def group_lasso_loss(lambda_vec, y_actual, y_predicted, weights):\n",
    "    \"\"\"Least squares with group LASSO\"\"\"\n",
    "    mse = tf.reduce_mean(tf.square(tf.subtract(y_actual, y_predicted)))\n",
    "    #l1_penalty = tf.reduce_mean(tf.matmul(tf.diag(lambda_vec), tf.abs(weights)))\n",
    "    l2_placeholder = tf.get_variable(shape=(1, 1), dtype=tf.float32, initializer=tf.zeros_initializer(), name=\"l2_placeholder\")\n",
    "    for key, value in weights.items():\n",
    "        l2_penalty = tf.norm(value, ord=1)\n",
    "        l2_placeholder = tf.add(l2_placeholder, l2_penalty)\n",
    "    \n",
    "# #     l2_penalty_list = []\n",
    "# #     for key, value in weights.items():\n",
    "# #         l2_penalty = tf.norm(value, ord=2)\n",
    "# #         l2_penalty_list.append(l2_penalty)\n",
    "    \n",
    "    l1_group_penalty = tf.add(mse, tf.matmul(lambda_val, l2_placeholder))\n",
    "    return l1_group_penalty\n",
    "    #return mse\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = group_lasso_loss(lambda_val, y_s1, logits, {\"s1\": w_s1, \"s2\": w_s2,\"s3\": w_s3,\"s4\": w_s4})\n",
    "#     loss = tf.reduce_mean(tf.square(tf.subtract(y_s1, logits)))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    accuracy = tf.reduce_mean(tf.square(tf.subtract(y_s1, logits)))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 1.6027742e-07\n",
      "1 Train accuracy: 2.1021752e-07\n",
      "2 Train accuracy: 2.0110878e-07\n",
      "3 Train accuracy: 5.9251414e-07\n",
      "4 Train accuracy: 6.632441e-07\n",
      "5 Train accuracy: 5.0787264e-07\n",
      "6 Train accuracy: 2.8612297e-07\n",
      "7 Train accuracy: 6.1687047e-07\n",
      "8 Train accuracy: 2.0011007e-07\n",
      "9 Train accuracy: 2.6221025e-08\n",
      "10 Train accuracy: 2.1047657e-09\n",
      "11 Train accuracy: 5.424381e-10\n",
      "12 Train accuracy: 2.7421607e-09\n",
      "13 Train accuracy: 6.1991288e-09\n",
      "14 Train accuracy: 1.12457474e-08\n",
      "15 Train accuracy: 1.8569994e-08\n",
      "16 Train accuracy: 2.8297165e-08\n",
      "17 Train accuracy: 3.980053e-08\n",
      "18 Train accuracy: 5.1904593e-08\n",
      "19 Train accuracy: 6.315124e-08\n",
      "20 Train accuracy: 7.268952e-08\n",
      "21 Train accuracy: 8.406254e-08\n",
      "22 Train accuracy: 8.886886e-08\n",
      "23 Train accuracy: 9.124729e-08\n",
      "24 Train accuracy: 9.274517e-08\n",
      "25 Train accuracy: 9.359012e-08\n",
      "26 Train accuracy: 9.3953545e-08\n",
      "27 Train accuracy: 9.3955755e-08\n",
      "28 Train accuracy: 9.367497e-08\n",
      "29 Train accuracy: 9.3167486e-08\n",
      "30 Train accuracy: 9.2484186e-08\n",
      "31 Train accuracy: 9.167971e-08\n",
      "32 Train accuracy: 9.0812684e-08\n",
      "33 Train accuracy: 8.995361e-08\n",
      "34 Train accuracy: 8.9181235e-08\n",
      "35 Train accuracy: 8.8574225e-08\n",
      "36 Train accuracy: 8.8217156e-08\n",
      "37 Train accuracy: 8.8180954e-08\n",
      "38 Train accuracy: 8.852013e-08\n",
      "39 Train accuracy: 8.926442e-08\n",
      "40 Train accuracy: 9.041244e-08\n",
      "41 Train accuracy: 9.1929785e-08\n",
      "42 Train accuracy: 9.37517e-08\n",
      "43 Train accuracy: 9.579067e-08\n",
      "44 Train accuracy: 9.794471e-08\n",
      "45 Train accuracy: 1.0011247e-07\n",
      "46 Train accuracy: 1.0220136e-07\n",
      "47 Train accuracy: 1.0413726e-07\n",
      "48 Train accuracy: 1.0586701e-07\n",
      "49 Train accuracy: 1.0735411e-07\n",
      "50 Train accuracy: 1.08593305e-07\n",
      "51 Train accuracy: 1.0958392e-07\n",
      "52 Train accuracy: 1.10345e-07\n",
      "53 Train accuracy: 1.1089882e-07\n",
      "54 Train accuracy: 1.1127498e-07\n",
      "55 Train accuracy: 1.11506715e-07\n",
      "56 Train accuracy: 1.1162346e-07\n",
      "57 Train accuracy: 1.1165128e-07\n",
      "58 Train accuracy: 1.1162141e-07\n",
      "59 Train accuracy: 1.1155245e-07\n",
      "60 Train accuracy: 1.1145888e-07\n",
      "61 Train accuracy: 1.1135552e-07\n",
      "62 Train accuracy: 1.1124972e-07\n",
      "63 Train accuracy: 1.111462e-07\n",
      "64 Train accuracy: 1.110488e-07\n",
      "65 Train accuracy: 1.1095882e-07\n",
      "66 Train accuracy: 1.1087626e-07\n",
      "67 Train accuracy: 1.10800286e-07\n",
      "68 Train accuracy: 1.1073043e-07\n",
      "69 Train accuracy: 1.1066662e-07\n",
      "70 Train accuracy: 1.10608504e-07\n",
      "71 Train accuracy: 1.1055535e-07\n",
      "72 Train accuracy: 1.1050771e-07\n",
      "73 Train accuracy: 1.1046441e-07\n",
      "74 Train accuracy: 1.1042314e-07\n",
      "75 Train accuracy: 1.1038594e-07\n",
      "76 Train accuracy: 1.1035257e-07\n",
      "77 Train accuracy: 1.1032226e-07\n",
      "78 Train accuracy: 1.1029383e-07\n",
      "79 Train accuracy: 1.1026739e-07\n",
      "80 Train accuracy: 1.1024447e-07\n",
      "81 Train accuracy: 1.1022363e-07\n",
      "82 Train accuracy: 1.1020432e-07\n",
      "83 Train accuracy: 1.10184445e-07\n",
      "84 Train accuracy: 1.1016972e-07\n",
      "85 Train accuracy: 1.1015362e-07\n",
      "86 Train accuracy: 1.1014141e-07\n",
      "87 Train accuracy: 1.10128404e-07\n",
      "88 Train accuracy: 1.101166e-07\n",
      "89 Train accuracy: 1.1010645e-07\n",
      "90 Train accuracy: 1.1009755e-07\n",
      "91 Train accuracy: 1.1008889e-07\n",
      "92 Train accuracy: 1.1008019e-07\n",
      "93 Train accuracy: 1.10073515e-07\n",
      "94 Train accuracy: 1.100666e-07\n",
      "95 Train accuracy: 1.1006128e-07\n",
      "96 Train accuracy: 1.10055474e-07\n",
      "97 Train accuracy: 1.100501e-07\n",
      "98 Train accuracy: 1.1004598e-07\n",
      "99 Train accuracy: 1.10041306e-07\n",
      "100 Train accuracy: 1.1003702e-07\n",
      "101 Train accuracy: 1.10033525e-07\n",
      "102 Train accuracy: 1.10030314e-07\n",
      "103 Train accuracy: 1.1002785e-07\n",
      "104 Train accuracy: 1.1002531e-07\n",
      "105 Train accuracy: 1.10022235e-07\n",
      "106 Train accuracy: 1.1001986e-07\n",
      "107 Train accuracy: 1.1001797e-07\n",
      "108 Train accuracy: 1.1001745e-07\n",
      "109 Train accuracy: 1.100149e-07\n",
      "110 Train accuracy: 1.1001243e-07\n",
      "111 Train accuracy: 1.1001166e-07\n",
      "112 Train accuracy: 1.10010525e-07\n",
      "113 Train accuracy: 1.10009374e-07\n",
      "114 Train accuracy: 1.10008564e-07\n",
      "115 Train accuracy: 1.1000801e-07\n",
      "116 Train accuracy: 1.10006795e-07\n",
      "117 Train accuracy: 1.1000566e-07\n",
      "118 Train accuracy: 1.1000446e-07\n",
      "119 Train accuracy: 1.1000382e-07\n",
      "120 Train accuracy: 1.10003214e-07\n",
      "121 Train accuracy: 1.1000206e-07\n",
      "122 Train accuracy: 1.1000206e-07\n",
      "123 Train accuracy: 1.1000261e-07\n",
      "124 Train accuracy: 1.1000072e-07\n",
      "125 Train accuracy: 1.1000072e-07\n",
      "126 Train accuracy: 1.1000129e-07\n",
      "127 Train accuracy: 1.1000072e-07\n",
      "128 Train accuracy: 1.0999942e-07\n",
      "129 Train accuracy: 1.0999942e-07\n",
      "130 Train accuracy: 1.0999942e-07\n",
      "131 Train accuracy: 1.0999894e-07\n",
      "132 Train accuracy: 1.0999894e-07\n",
      "133 Train accuracy: 1.09998204e-07\n",
      "134 Train accuracy: 1.09998204e-07\n",
      "135 Train accuracy: 1.0999758e-07\n",
      "136 Train accuracy: 1.0999894e-07\n",
      "137 Train accuracy: 1.09998204e-07\n",
      "138 Train accuracy: 1.09998204e-07\n",
      "139 Train accuracy: 1.0999758e-07\n",
      "140 Train accuracy: 1.09998204e-07\n",
      "141 Train accuracy: 1.0999758e-07\n",
      "142 Train accuracy: 1.09998204e-07\n",
      "143 Train accuracy: 1.09998204e-07\n",
      "144 Train accuracy: 1.0999758e-07\n",
      "145 Train accuracy: 1.0999758e-07\n",
      "146 Train accuracy: 1.0999758e-07\n",
      "147 Train accuracy: 1.09997124e-07\n",
      "148 Train accuracy: 1.09997124e-07\n",
      "149 Train accuracy: 1.09997124e-07\n",
      "150 Train accuracy: 1.09997124e-07\n",
      "151 Train accuracy: 1.09997124e-07\n",
      "152 Train accuracy: 1.09997124e-07\n",
      "153 Train accuracy: 1.09997124e-07\n",
      "154 Train accuracy: 1.09997124e-07\n",
      "155 Train accuracy: 1.09997124e-07\n",
      "156 Train accuracy: 1.09997124e-07\n",
      "157 Train accuracy: 1.09997124e-07\n",
      "158 Train accuracy: 1.09997124e-07\n",
      "159 Train accuracy: 1.09997124e-07\n",
      "160 Train accuracy: 1.09997124e-07\n",
      "161 Train accuracy: 1.09996414e-07\n",
      "162 Train accuracy: 1.09997124e-07\n",
      "163 Train accuracy: 1.09997124e-07\n",
      "164 Train accuracy: 1.09997124e-07\n",
      "165 Train accuracy: 1.09997124e-07\n",
      "166 Train accuracy: 1.09997124e-07\n",
      "167 Train accuracy: 1.09997124e-07\n",
      "168 Train accuracy: 1.09997124e-07\n",
      "169 Train accuracy: 1.09997124e-07\n",
      "170 Train accuracy: 1.09997124e-07\n",
      "171 Train accuracy: 1.09997124e-07\n",
      "172 Train accuracy: 1.09997124e-07\n",
      "173 Train accuracy: 1.09997124e-07\n",
      "174 Train accuracy: 1.09997124e-07\n",
      "175 Train accuracy: 1.09997124e-07\n",
      "176 Train accuracy: 1.09997124e-07\n",
      "177 Train accuracy: 1.09997124e-07\n",
      "178 Train accuracy: 1.09997124e-07\n",
      "179 Train accuracy: 1.09997124e-07\n",
      "180 Train accuracy: 1.09997124e-07\n",
      "181 Train accuracy: 1.09997124e-07\n",
      "182 Train accuracy: 1.09997124e-07\n",
      "183 Train accuracy: 1.09997124e-07\n",
      "184 Train accuracy: 1.09997124e-07\n",
      "185 Train accuracy: 1.09997124e-07\n",
      "186 Train accuracy: 1.09997124e-07\n",
      "187 Train accuracy: 1.09997124e-07\n",
      "188 Train accuracy: 1.09997124e-07\n",
      "189 Train accuracy: 1.09997124e-07\n",
      "190 Train accuracy: 1.09997124e-07\n",
      "191 Train accuracy: 1.09997124e-07\n",
      "192 Train accuracy: 1.09997124e-07\n",
      "193 Train accuracy: 1.09997124e-07\n",
      "194 Train accuracy: 1.09997124e-07\n",
      "195 Train accuracy: 1.09997124e-07\n",
      "196 Train accuracy: 1.09997124e-07\n",
      "197 Train accuracy: 1.09997124e-07\n",
      "198 Train accuracy: 1.09997124e-07\n",
      "199 Train accuracy: 1.09997124e-07\n",
      "[[-2.43377610e-04  6.00146705e-05  8.87002971e-05 -4.86491372e-05]\n",
      " [-2.84901966e-04 -1.45272250e-04 -7.81750277e-05 -2.88040872e-04]\n",
      " [-2.92940531e-05 -1.69408268e-05 -1.57061819e-04  1.20322875e-04]\n",
      " [ 1.53483255e-04  1.11119676e-04 -2.63296213e-04  1.98596652e-04]\n",
      " [-3.34619108e-06 -7.12583787e-05 -1.65073230e-04 -7.82390271e-06]\n",
      " [-1.60279073e-04 -1.01727179e-04 -9.76542578e-05  1.36887538e-04]\n",
      " [ 7.55171859e-05 -8.81990200e-05 -6.88057844e-05  5.23744035e-04]\n",
      " [ 5.11834151e-05 -7.34328642e-05 -7.21867764e-06 -7.60839248e-06]\n",
      " [-4.92334948e-04  8.62169400e-05 -6.44361935e-05  4.75064880e-05]\n",
      " [-3.87447872e-05  9.56844233e-05 -1.05357176e-05  2.42812879e-04]\n",
      " [ 5.00422830e-05  1.46158607e-04  1.11998663e-04  2.46618089e-04]\n",
      " [ 1.78798407e-04 -3.61007624e-05 -1.38422387e-04  1.70840503e-04]\n",
      " [-2.70531018e-06  9.08179936e-05  7.52523047e-05 -1.20905497e-06]\n",
      " [ 9.61787464e-06  5.10215250e-06  3.14386562e-04  7.86154851e-06]]\n",
      "[[ 1.39754586e-04  1.91419211e-04  1.20533339e-04 -3.38002574e-05]\n",
      " [-6.34612588e-05 -5.82252615e-05  8.93851320e-05 -1.76523492e-04]\n",
      " [-8.58766361e-05 -2.63285678e-04 -4.55701811e-05 -4.27716732e-04]\n",
      " [ 3.77385877e-05  2.17127832e-04 -7.58510723e-05 -7.68732352e-05]\n",
      " [-3.06158690e-05 -1.26480227e-04  1.11963920e-04  2.80425993e-05]\n",
      " [-1.58351904e-05  1.82150950e-04  7.61086267e-06 -2.63366674e-05]\n",
      " [-2.81145214e-04 -4.07043844e-05  3.29837931e-05  4.11474939e-05]\n",
      " [ 6.47600173e-05 -8.35748069e-05  1.96635010e-04  2.13551320e-05]\n",
      " [ 1.12220587e-05 -2.46933312e-04 -1.12270165e-04 -3.91802714e-05]\n",
      " [-8.86987837e-05 -4.18592135e-05  1.47133789e-04  1.18998280e-04]\n",
      " [-2.19174035e-06  2.94913407e-05  4.43791068e-05  1.58745388e-05]\n",
      " [ 1.46290404e-06  7.74682339e-05  1.75079593e-04 -3.04336281e-05]\n",
      " [-3.17447775e-05  9.96479866e-05 -5.99009290e-05 -3.24176362e-05]\n",
      " [ 8.61700100e-05 -9.93042486e-05 -3.00082756e-05 -1.11255657e-04]]\n",
      "[[-4.74025946e-05 -1.92602765e-05 -1.88585283e-04 -4.06015170e-05]\n",
      " [ 1.59901218e-04 -4.03381127e-04 -1.20981495e-05  2.03237068e-04]\n",
      " [ 1.74365763e-04 -2.11106060e-04  2.02957890e-04  9.28459340e-05]\n",
      " [ 2.85073838e-06  3.74503179e-05  9.18935475e-05 -1.48820545e-04]\n",
      " [-6.01732463e-05  7.23928097e-05  3.59567639e-06  2.52889367e-05]\n",
      " [ 2.21976734e-05 -1.51831249e-04  6.12931326e-05  8.68886491e-06]\n",
      " [-1.00516409e-04  1.95086774e-04  9.92661444e-06 -8.80805310e-05]\n",
      " [ 5.76674938e-06 -1.92599036e-05  1.34596849e-06 -5.05162971e-05]\n",
      " [ 1.19954377e-04  2.18595123e-05  1.53330577e-04 -3.61234925e-05]\n",
      " [ 8.07996548e-05  1.39631215e-04  8.80251901e-06 -6.26488647e-04]\n",
      " [ 1.27011939e-04  7.70073602e-05 -1.70669315e-04  2.29637924e-04]\n",
      " [-5.15132197e-05 -2.01386269e-04 -2.45741903e-05 -2.38753550e-04]\n",
      " [ 1.66103942e-04  6.20710489e-05 -7.75705048e-05 -3.83490260e-04]\n",
      " [ 6.89359003e-05 -1.13325004e-04  1.75221561e-04 -1.31074688e-04]]\n",
      "[[ 1.0104557e-04 -1.8594164e-04 -7.9270721e-06 -3.8067818e-05]\n",
      " [-1.2627970e-04  2.8024969e-04  2.2737440e-04 -4.7666419e-04]\n",
      " [-1.0935247e-04 -1.6361799e-04 -2.0991152e-05 -1.1638873e-05]\n",
      " [-2.5196743e-04 -1.4858047e-04  1.4283255e-04  1.7327179e-04]\n",
      " [ 3.7830978e-07  1.7240450e-04 -1.4958782e-04  2.3787253e-04]\n",
      " [-4.8998863e-05 -1.8848755e-04 -1.0952865e-04 -1.5674171e-04]\n",
      " [ 1.2273010e-04  9.5073978e-05 -8.3645064e-06  1.3787931e-04]\n",
      " [-6.6931359e-05 -5.3110023e-05  1.2468171e-04  8.9597212e-05]\n",
      " [ 1.2896919e-04  1.4716260e-04 -1.3507654e-04 -1.6520951e-04]\n",
      " [ 9.4151925e-05 -3.4827048e-05 -3.8697349e-04 -1.2565401e-05]\n",
      " [-2.9897477e-05  1.6857701e-04 -1.5961552e-05  4.6969963e-05]\n",
      " [ 3.9534693e-04 -1.2052289e-04 -1.4891403e-04 -1.2981379e-04]\n",
      " [ 2.6432556e-04 -2.3274624e-04  3.1178191e-05 -4.7488989e-05]\n",
      " [-3.3342853e-04 -2.6753034e-05  1.6951334e-04 -4.9092050e-05]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "# tf.summary.scalar('number_of_epochs', n_epochs)\n",
    "# tf.summary.scalar('batch_size', batch_size)\n",
    "        \n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "train_writer = tf.summary.FileWriter('./Pgraphs/train', graph = tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     merged = tf.summary.merge_all()\n",
    "#     train_writer = tf.summary.FileWriter('./graphs/train', sess.graph)\n",
    "#     val_writer = tf.summary.FileWriter('./graphs/val')\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(X_s1_values.shape[0] // batch_size):\n",
    "#             print(i)\n",
    "#             print(X_s1_values[i*batch_size:(i+1)*batch_size])\n",
    "            X_batch_s1, y_batch_s1 = X_s1_values[i*batch_size:(i+1)*batch_size], Y_s1_values[i*batch_size:(i+1)*batch_size]\n",
    "            X_batch_s2 = X_s2_values[i*batch_size:(i+1)*batch_size]           \n",
    "            X_batch_s3 = X_s3_values[i*batch_size:(i+1)*batch_size]           \n",
    "            X_batch_s4 = X_s4_values[i*batch_size:(i+1)*batch_size]           \n",
    "        \n",
    "            _, _ = sess.run([training_op, accuracy], feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, X_s3:X_batch_s3, X_s4:X_batch_s4, y_s1:y_batch_s1})\n",
    "#         summary_train = sess.run(merged, feed_dict = {X:X_batch, y:y_batch})\n",
    "#         train_writer.add_summary(summary_train, epoch)\n",
    "#         summary_val = sess.run(merged, feed_dict={X: X_val.reshape(-1, n_inputs), y: Y_val})\n",
    "#         val_writer.add_summary(summary_val, epoch)\n",
    "            \n",
    "        acc_train = accuracy.eval(feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, X_s3:X_batch_s3, X_s4:X_batch_s4, y_s1:y_batch_s1})\n",
    "    \n",
    "#         print(sess.run(b_s1, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         print(sess.run(s1_hidden1, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         print(sess.run(logits, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         print(sess.run(w_s1, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         print(sess.run(X_s1, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         print(sess.run(b_s1, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, y_s1:y_batch_s1}))\n",
    "#         acc_val = accuracy.eval(feed_dict= {X: X_val.reshape(-1, n_inputs), y: Y_val})\n",
    "        logit = sess.run(logits, feed_dict = {X_s1:X_batch_s1, X_s2:X_batch_s2, X_s3:X_batch_s3, X_s4:X_batch_s4, y_s1:y_batch_s1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train)#, \"Val accuracy:\", acc_val)\n",
    "        \n",
    "    print(sess.run(w_s1))\n",
    "    print(sess.run(w_s2))\n",
    "    print(sess.run(w_s3))\n",
    "    print(sess.run(w_s4))\n",
    "    save_path = saver.save(sess, \"./graphs/my_model_final.ckpt\")\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "#     X_new_scaled = test.reshape(-1, n_inputs)\n",
    "#     Z = logits.eval(feed_dict = {X: X_new_scaled})\n",
    "#     y_pred = np.argmax(Z, axis = 1)\n",
    "\n",
    "# print(\"Predicted classes:\", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
