{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moddate</th>\n",
       "      <th>AAPL_VWP_t_14</th>\n",
       "      <th>MSFT_VWP_t_14</th>\n",
       "      <th>FB_VWP_t_14</th>\n",
       "      <th>TWTR_VWP_t_14</th>\n",
       "      <th>AAPL_VWP_t_13</th>\n",
       "      <th>MSFT_VWP_t_13</th>\n",
       "      <th>FB_VWP_t_13</th>\n",
       "      <th>TWTR_VWP_t_13</th>\n",
       "      <th>AAPL_VWP_t_12</th>\n",
       "      <th>...</th>\n",
       "      <th>FB_VWP_t_2</th>\n",
       "      <th>TWTR_VWP_t_2</th>\n",
       "      <th>AAPL_VWP_t_1</th>\n",
       "      <th>MSFT_VWP_t_1</th>\n",
       "      <th>FB_VWP_t_1</th>\n",
       "      <th>TWTR_VWP_t_1</th>\n",
       "      <th>AAPL_VWP</th>\n",
       "      <th>MSFT_VWP</th>\n",
       "      <th>FB_VWP</th>\n",
       "      <th>TWTR_VWP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03 18:00:00</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-03 19:00:00</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03 20:00:00</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.009829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-03 21:00:00</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.009831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-03 22:00:00</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.009834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               moddate  AAPL_VWP_t_14  MSFT_VWP_t_14  FB_VWP_t_14  \\\n",
       "0  2017-01-03 18:00:00       0.008251       0.009330     0.007926   \n",
       "1  2017-01-03 19:00:00       0.008258       0.009330     0.007946   \n",
       "2  2017-01-03 20:00:00       0.008258       0.009334     0.007939   \n",
       "3  2017-01-03 21:00:00       0.008267       0.009351     0.007956   \n",
       "4  2017-01-03 22:00:00       0.008273       0.009342     0.007944   \n",
       "\n",
       "   TWTR_VWP_t_14  AAPL_VWP_t_13  MSFT_VWP_t_13  FB_VWP_t_13  TWTR_VWP_t_13  \\\n",
       "0       0.009827       0.008258       0.009330     0.007946       0.009876   \n",
       "1       0.009877       0.008258       0.009333     0.007939       0.009870   \n",
       "2       0.009871       0.008267       0.009351     0.007955       0.009848   \n",
       "3       0.009848       0.008273       0.009342     0.007944       0.009854   \n",
       "4       0.009855       0.008267       0.009313     0.007932       0.009803   \n",
       "\n",
       "   AAPL_VWP_t_12    ...     FB_VWP_t_2  TWTR_VWP_t_2  AAPL_VWP_t_1  \\\n",
       "0       0.008258    ...       0.007981      0.009851      0.008257   \n",
       "1       0.008267    ...       0.007980      0.009847      0.008254   \n",
       "2       0.008273    ...       0.007978      0.009840      0.008253   \n",
       "3       0.008267    ...       0.007973      0.009839      0.008257   \n",
       "4       0.008227    ...       0.007970      0.009830      0.008260   \n",
       "\n",
       "   MSFT_VWP_t_1  FB_VWP_t_1  TWTR_VWP_t_1  AAPL_VWP  MSFT_VWP    FB_VWP  \\\n",
       "0      0.009302    0.007980      0.009847  0.008254  0.009279  0.007977   \n",
       "1      0.009280    0.007978      0.009839  0.008253  0.009280  0.007972   \n",
       "2      0.009281    0.007973      0.009838  0.008257  0.009283  0.007969   \n",
       "3      0.009283    0.007969      0.009829  0.008259  0.009287  0.007976   \n",
       "4      0.009288    0.007976      0.009832  0.008262  0.009292  0.007982   \n",
       "\n",
       "   TWTR_VWP  \n",
       "0  0.009839  \n",
       "1  0.009838  \n",
       "2  0.009829  \n",
       "3  0.009831  \n",
       "4  0.009834  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8642, 61)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('tech_60m.csv')\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_VWP_t_14</th>\n",
       "      <th>MSFT_VWP_t_14</th>\n",
       "      <th>FB_VWP_t_14</th>\n",
       "      <th>TWTR_VWP_t_14</th>\n",
       "      <th>AAPL_VWP_t_13</th>\n",
       "      <th>MSFT_VWP_t_13</th>\n",
       "      <th>FB_VWP_t_13</th>\n",
       "      <th>TWTR_VWP_t_13</th>\n",
       "      <th>AAPL_VWP_t_12</th>\n",
       "      <th>MSFT_VWP_t_12</th>\n",
       "      <th>...</th>\n",
       "      <th>FB_VWP_t_3</th>\n",
       "      <th>TWTR_VWP_t_3</th>\n",
       "      <th>AAPL_VWP_t_2</th>\n",
       "      <th>MSFT_VWP_t_2</th>\n",
       "      <th>FB_VWP_t_2</th>\n",
       "      <th>TWTR_VWP_t_2</th>\n",
       "      <th>AAPL_VWP_t_1</th>\n",
       "      <th>MSFT_VWP_t_1</th>\n",
       "      <th>FB_VWP_t_1</th>\n",
       "      <th>TWTR_VWP_t_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.009829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.009832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AAPL_VWP_t_14  MSFT_VWP_t_14  FB_VWP_t_14  TWTR_VWP_t_14  AAPL_VWP_t_13  \\\n",
       "0       0.008251       0.009330     0.007926       0.009827       0.008258   \n",
       "1       0.008258       0.009330     0.007946       0.009877       0.008258   \n",
       "2       0.008258       0.009334     0.007939       0.009871       0.008267   \n",
       "3       0.008267       0.009351     0.007956       0.009848       0.008273   \n",
       "4       0.008273       0.009342     0.007944       0.009855       0.008267   \n",
       "\n",
       "   MSFT_VWP_t_13  FB_VWP_t_13  TWTR_VWP_t_13  AAPL_VWP_t_12  MSFT_VWP_t_12  \\\n",
       "0       0.009330     0.007946       0.009876       0.008258       0.009333   \n",
       "1       0.009333     0.007939       0.009870       0.008267       0.009350   \n",
       "2       0.009351     0.007955       0.009848       0.008273       0.009342   \n",
       "3       0.009342     0.007944       0.009854       0.008267       0.009313   \n",
       "4       0.009313     0.007932       0.009803       0.008227       0.009312   \n",
       "\n",
       "       ...       FB_VWP_t_3  TWTR_VWP_t_3  AAPL_VWP_t_2  MSFT_VWP_t_2  \\\n",
       "0      ...         0.007962      0.009804      0.008263      0.009306   \n",
       "1      ...         0.007981      0.009851      0.008258      0.009303   \n",
       "2      ...         0.007980      0.009848      0.008255      0.009280   \n",
       "3      ...         0.007978      0.009840      0.008254      0.009281   \n",
       "4      ...         0.007973      0.009839      0.008257      0.009284   \n",
       "\n",
       "   FB_VWP_t_2  TWTR_VWP_t_2  AAPL_VWP_t_1  MSFT_VWP_t_1  FB_VWP_t_1  \\\n",
       "0    0.007981      0.009851      0.008257      0.009302    0.007980   \n",
       "1    0.007980      0.009847      0.008254      0.009280    0.007978   \n",
       "2    0.007978      0.009840      0.008253      0.009281    0.007973   \n",
       "3    0.007973      0.009839      0.008257      0.009283    0.007969   \n",
       "4    0.007970      0.009830      0.008260      0.009288    0.007976   \n",
       "\n",
       "   TWTR_VWP_t_1  \n",
       "0      0.009847  \n",
       "1      0.009839  \n",
       "2      0.009838  \n",
       "3      0.009829  \n",
       "4      0.009832  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8642, 56)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00825107, 0.00933008, 0.00792621, ..., 0.00930246, 0.00797956,\n",
       "        0.00984677],\n",
       "       [0.0082581 , 0.00933033, 0.00794631, ..., 0.00927969, 0.00797767,\n",
       "        0.00983935],\n",
       "       [0.00825817, 0.00933369, 0.00793947, ..., 0.00928062, 0.00797273,\n",
       "        0.00983821],\n",
       "       ...,\n",
       "       [0.01215699, 0.01274062, 0.01216899, ..., 0.01272049, 0.01205027,\n",
       "        0.01438101],\n",
       "       [0.01215659, 0.01273626, 0.01217158, ..., 0.01271451, 0.01203102,\n",
       "        0.01437984],\n",
       "       [0.0121563 , 0.01272719, 0.01217946, ..., 0.01269072, 0.01202995,\n",
       "        0.01436187]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get features and convert to array so that we can feed into model\n",
    "data_x = data.iloc[:, 1:57]\n",
    "data_x.head()\n",
    "data_x.shape\n",
    "data_x = data_x.values\n",
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.009279\n",
       "1    0.009280\n",
       "2    0.009283\n",
       "3    0.009287\n",
       "4    0.009292\n",
       "Name: MSFT_VWP, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8642,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.00927934, 0.00928027, 0.00928309, ..., 0.01271404, 0.01269025,\n",
       "       0.01270244])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get actual y values and convert to array \n",
    "data_y = data.iloc[:, 58]\n",
    "data_y.head()\n",
    "data_y.shape\n",
    "data_y = data_y.values\n",
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 20\n",
    "lambda_value = 0.1\n",
    "\n",
    "# network parameters\n",
    "n_inputs = 4\n",
    "n_hidden = 128\n",
    "n_outputs = 1\n",
    "timesteps = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(lambda_val, y_actual, y_predicted, weights_dict, norm_order=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates the loss based on MSE and group penalty.\n",
    "    MSE is calculated using the actual and predicted output values.\n",
    "    The group penalty is the l2 regularization applied on the weigths.\n",
    "    \n",
    "    Inputs:\n",
    "    lambda_val: value of regularization parameters\n",
    "    y_actual: tensor of actual output values\n",
    "    y_predicted: tensor of predicted output values\n",
    "    weights_dict: dictionary of weights\n",
    "    norm_order: the order for penality, 1 = l1, 2 = l2 etc. Default is 2.\n",
    "    \n",
    "    Output: The calculated loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    mse = tf.reduce_mean(tf.square(tf.subtract(y_actual, y_predicted)))\n",
    "    \n",
    "    penalty_placeholder = tf.get_variable(shape=(1, 1), dtype=tf.float32, initializer=tf.zeros_initializer(), name=\"penalty_placeholder\")\n",
    "    \n",
    "    # calculate the norm on each weight matrix\n",
    "    for key, value in weights_dict.items():\n",
    "        penalty = tf.norm(value, ord=norm_order)\n",
    "        penalty_placeholder = tf.add(penalty_placeholder, penalty)\n",
    "    \n",
    "    # the combined penalty = mse + lambda * norm(weights)\n",
    "    loss = tf.add(mse, tf.matmul(lambda_val, penalty_placeholder))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_op:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, n_inputs], name=\"X\")\n",
    "    Y = tf.placeholder(\"float\", [None], name=\"Y\")\n",
    "\n",
    "    lambda_val = tf.fill((1, 1), lambda_value, name=\"lambda\")\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_outputs]), name=\"weights_out\")\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_outputs]), name=\"biases_out\")\n",
    "    }\n",
    "\n",
    "    weights_dict = data = {k: [] for k in range(n_inputs)}\n",
    "\n",
    "    # loss and optimizer\n",
    "\n",
    "    # current data input shape: (batch_size, timesteps, n_input)\n",
    "    # required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "    x = tf.unstack(X, timesteps, 1) # # tensors of shape (batch_size, n_inputs)\n",
    "\n",
    "    # define lstm cell\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    logits = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    # get the difference weight components\n",
    "    weights = lstm_cell.get_weights()[0]\n",
    "    w_i, w_C, w_f, w_o = np.split(weights, 4, axis=1)\n",
    "\n",
    "    w_xi = w_i[:n_inputs, :]\n",
    "    w_hi = w_i[n_inputs:, :]\n",
    "\n",
    "    w_xC = w_C[:n_inputs, :]\n",
    "    w_hC = w_C[n_inputs:, :]\n",
    "\n",
    "    w_xf = w_f[:n_inputs, :]\n",
    "    w_hf = w_f[n_inputs:, :]\n",
    "\n",
    "    w_xo = w_o[:n_inputs, :]\n",
    "    w_ho = w_o[n_inputs:, :]\n",
    "\n",
    "    # concatenate the weight components and transpose so that we have the stocks as columns\n",
    "    W = np.concatenate((w_xi, w_xC, w_xf, w_xo), axis = 1)\n",
    "    W_transpose = W.T\n",
    "\n",
    "    # get the weights for each stock and store in weight_dict\n",
    "    for i in range(n_inputs):\n",
    "        weights_dict[i] = W_transpose[:, i]\n",
    "\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    loss_op = loss(lambda_val, Y, logits, weights_dict)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # evaluation\n",
    "    accuracy = tf.reduce_mean(tf.square(tf.subtract(Y, logits)))\n",
    "    \n",
    "    # initialization\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # summary\n",
    "    tf.summary.scalar('loss_op', loss_op[0][0])\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # tensorboard graph\n",
    "    summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(data_x.shape[0] // batch_size):\n",
    "            batch_x, batch_y = data_x[i*batch_size:(i+1)*batch_size], data_y[i*batch_size:(i+1)*batch_size]\n",
    "            batch_x = batch_x.reshape((batch_size, timesteps, n_inputs))\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "        # loss and accuracy for epoch\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # add summary for epoch\n",
    "        summary_epoch = sess.run(summary, feed_dict = {X:batch_x, Y:batch_y})\n",
    "        writer.add_summary(summary_epoch, epoch)\n",
    "\n",
    "        # display loss and accuracy for each display_step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Step: \", str(epoch), \"Minibatch Loss: \", loss, \", Training Accuracy: \", acc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the weights\n",
    "W_abs = abs(W_transpose)\n",
    "W_abs.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_abs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (TF noGPU)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
